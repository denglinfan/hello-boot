 本周给大家介绍一个最近非常流行的算法lightGBM，该算法微软在GitHub上开源后，迅速得到了广泛的关注，在GitHub上的star数迅速达到了5000+，fork数也超过了1000。其论文发表在2017年nips上，引起了业内的广泛关注。
         LightGBM作为boosting家族中最近兴起的算法，针对之前算法的问题做出了很大的改变，在分类和回归问题上具有显著的效果。

具体如下：
1. foreword
         GBDT全称梯度下降树，在传统机器学习算法里面是对真实分布拟合的最好的几种算法之一，在前几年深度学习还没有大行其道之前，GBDT在各种竞赛是大放异彩。原因大概有几个，一是效果确实挺不错。二是即可以用于分类也可以用于回归。三是可以筛选特征。其具体执行流程如下：

XgBoost在GBDT的基础上再次升级改良，也迅速成为各种竞赛的宠儿，并且也取得了非常显著的效果。其执行流程与GBDT类似，也有其缺点，或者说不足之处：
1、每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。
2、预排序方法（pre-sorted）：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。其次时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。
3、对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。

2. lightGBM简介
顾名思义，lightGBM包含两个关键点：light即轻量级，GBM 梯度提升机。LightGBM 是一个梯度 boosting 框架，使用基于学习算法的决策树。它可以说是分布式的，高效的，有以下优势：更快的训练速度和效率、低内存使用、更高的准确率、支持并行化学习、可处理大规模数据。
lightGBM作者们构建新算法时着重瞄准了XgBoost的不足之处。解决了什么问题，那么原来模型没解决就成了原模型的特点。lightGBM有两个主要特点：
1、基于Histogram的决策树算法
直方图算法的基本思想：先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。遍历数据时，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。
2、带深度限制的Leaf-wise的叶子生长策略
                   Leaf-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。
Leaf-wise则是一种更为高效的策略：每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。
Leaf-wise的缺点：可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合。

3．相关资料
         中文文档：http://lightgbm.apachecn.org/cn/latest/index.html
         GitHup地址：https://github.com/Microsoft/LightGBM
         论文地址：http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf

